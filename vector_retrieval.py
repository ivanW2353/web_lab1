import time
from typing import List, Dict, Tuple
import os
import json
import hashlib
from tqdm import tqdm
import math
from collections import defaultdict

class OptimizedTFIDF:
    """ä¼˜åŒ–çš„TF-IDFå®žçŽ°ï¼Œä½¿ç”¨ç¨€ç–å‘é‡å‡å°‘å†…å­˜ä½¿ç”¨"""
    
    def __init__(self, cache_dir: str = "Meetup/cache"):
        self.vocab = {}
        self.doc_freq = {}
        self.doc_count = 0
        self.doc_vectors = {}  # ä½¿ç”¨ç¨€ç–è¡¨ç¤º: {doc_id: {term_idx: tfidf_value}}
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, documents: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŸºäºŽæ–‡æ¡£å†…å®¹"""
        # ä½¿ç”¨æ–‡æ¡£æ•°é‡å’Œå‰å‡ ä¸ªæ–‡æ¡£çš„å“ˆå¸Œæ¥ç”Ÿæˆç¼“å­˜é”®
        sample_content = "".join([doc[:100] for doc in documents[:10]]) + str(len(documents))
        return hashlib.md5(sample_content.encode()).hexdigest()
    
    def _get_cache_file(self, documents: List[str]) -> str:
        """èŽ·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self._get_cache_key(documents)
        return os.path.join(self.cache_dir, f"tfidf_optimized_{cache_key}.json")
    
    def load_from_cache(self, documents: List[str]) -> bool:
        """ä»Žç¼“å­˜åŠ è½½TF-IDFæ•°æ®"""
        cache_file = self._get_cache_file(documents)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'vocab' in data and 'doc_freq' in data and 'doc_vectors' in data:
                    self.vocab = data['vocab']
                    self.doc_freq = data['doc_freq']
                    self.doc_vectors = data['doc_vectors']
                    self.doc_count = data['doc_count']
                    
                    print(f"âœ… ä»Žç¼“å­˜åŠ è½½TF-IDFæ•°æ®æˆåŠŸ: è¯æ±‡è¡¨å¤§å° {len(self.vocab)}")
                    return True
            except Exception as e:
                print(f"âŒ åŠ è½½TF-IDFç¼“å­˜å¤±è´¥: {e}")
        
        return False
    
    def save_to_cache(self, documents: List[str]):
        """ä¿å­˜TF-IDFæ•°æ®åˆ°ç¼“å­˜"""
        cache_file = self._get_cache_file(documents)
        try:
            data = {
                'metadata': {
                    'document_count': self.doc_count,
                    'vocab_size': len(self.vocab),
                    'timestamp': time.time()
                },
                'vocab': self.vocab,
                'doc_freq': self.doc_freq,
                'doc_vectors': self.doc_vectors,
                'doc_count': self.doc_count
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, separators=(',', ':'))
            
        except Exception as e:
            print(f"âŒ ä¿å­˜TF-IDFç¼“å­˜å¤±è´¥: {e}")
    
    def fit_transform(self, documents: List[str], use_cache: bool = True, max_features: int = 50000) -> Dict[int, Dict[int, float]]:
        """è®­ç»ƒTF-IDFæ¨¡åž‹å¹¶è½¬æ¢æ–‡æ¡£ï¼Œä½¿ç”¨ç¨€ç–å‘é‡è¡¨ç¤º"""
        # å°è¯•ä»Žç¼“å­˜åŠ è½½
        if use_cache and self.load_from_cache(documents):
            return self.doc_vectors
            
        self.doc_count = len(documents)
        print(f"ðŸ“ˆ æž„å»º TF-IDF å‘é‡ (é™åˆ¶ç‰¹å¾æ•°: {max_features})...")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæž„å»ºè¯æ±‡è¡¨å’Œè®¡ç®—æ–‡æ¡£é¢‘çŽ‡ï¼ˆé™åˆ¶ç‰¹å¾æ•°ï¼‰
        print("ðŸ“ ç¬¬ä¸€é˜¶æ®µï¼šæž„å»ºè¯æ±‡è¡¨å’Œè®¡ç®—æ–‡æ¡£é¢‘çŽ‡")
        term_doc_freq = defaultdict(int)
        
        # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ–‡æ¡£
        df_progress = tqdm(
            documents,
            desc="ðŸ“Š è®¡ç®—è¯é¡¹é¢‘çŽ‡",
            total=len(documents),
            unit="æ–‡æ¡£",
            ncols=100
        )
        
        for doc in df_progress:
            terms = self._tokenize(doc)
            for term in set(terms):  # åªè®¡ç®—æ¯ä¸ªæ–‡æ¡£ä¸­æ¯ä¸ªè¯é¡¹ä¸€æ¬¡
                term_doc_freq[term] += 1
        
        df_progress.close()
        
        # é€‰æ‹©æœ€å¸¸ç”¨çš„è¯é¡¹ä½œä¸ºç‰¹å¾
        sorted_terms = sorted(term_doc_freq.items(), key=lambda x: x[1], reverse=True)
        selected_terms = [term for term, freq in sorted_terms[:max_features]]
        
        self.vocab = {term: idx for idx, term in enumerate(selected_terms)}
        self.doc_freq = {term: term_doc_freq[term] for term in selected_terms}
        
        print(f"âœ… è¯æ±‡è¡¨æž„å»ºå®Œæˆï¼Œé€‰æ‹© {len(self.vocab)} ä¸ªç‰¹å¾")
        
        # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—TF-IDFå‘é‡ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰
        print("ðŸ“ˆ ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—TF-IDFç¨€ç–å‘é‡")
        vectors = {}
        
        vector_progress = tqdm(
            enumerate(documents),
            desc="ðŸ”¢ è®¡ç®—æ–‡æ¡£å‘é‡",
            total=len(documents),
            unit="æ–‡æ¡£",
            ncols=100
        )
        
        for doc_id, doc in vector_progress:
            terms = self._tokenize(doc)
            term_count = len(terms)
            
            if term_count == 0:
                vectors[doc_id] = {}
                continue
            
            # è®¡ç®—è¯é¢‘
            tf_dict = defaultdict(int)
            for term in terms:
                if term in self.vocab:  # åªè€ƒè™‘åœ¨è¯æ±‡è¡¨ä¸­çš„è¯é¡¹
                    tf_dict[term] += 1
            
            # è®¡ç®—TF-IDFï¼ˆç¨€ç–è¡¨ç¤ºï¼‰
            sparse_vector = {}
            for term, count in tf_dict.items():
                if term in self.vocab:
                    idx = self.vocab[term]
                    tf = count / term_count
                    idf = math.log(self.doc_count / (self.doc_freq[term] + 1))
                    sparse_vector[idx] = tf * idf
            
            vectors[doc_id] = sparse_vector
        
        vector_progress.close()
        
        self.doc_vectors = vectors
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self.save_to_cache(documents)
        
        print(f"âœ… TF-IDFå‘é‡æž„å»ºå®Œæˆï¼Œä½¿ç”¨ç¨€ç–è¡¨ç¤º")
        return vectors
    
    def transform(self, texts: List[str]) -> List[Dict[int, float]]:
        """è½¬æ¢æ–°æ–‡æœ¬ä¸ºTF-IDFç¨€ç–å‘é‡"""
        vectors = []
        for text in texts:
            terms = self._tokenize(text)
            term_count = len(terms)
            
            if term_count == 0:
                vectors.append({})
                continue
            
            tf_dict = defaultdict(int)
            for term in terms:
                if term in self.vocab:
                    tf_dict[term] += 1
            
            sparse_vector = {}
            for term, count in tf_dict.items():
                if term in self.vocab:
                    idx = self.vocab[term]
                    tf = count / term_count
                    idf = math.log(self.doc_count / (self.doc_freq[term] + 1))
                    sparse_vector[idx] = tf * idf
            
            vectors.append(sparse_vector)
        
        return vectors
    
    def _tokenize(self, text: str) -> List[str]:
        """ä¼˜åŒ–çš„åˆ†è¯å‡½æ•°"""
        if not text:
            return []
        
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # ä½¿ç”¨ç®€å•çš„åˆ†è¯ï¼Œç§»é™¤éžå­—æ¯å­—ç¬¦
        words = []
        current_word = []
        
        for char in text:
            if char.isalpha():
                current_word.append(char)
            elif current_word:
                word = ''.join(current_word)
                if len(word) > 2:  # åªä¿ç•™é•¿åº¦å¤§äºŽ2çš„è¯
                    words.append(word)
                current_word = []
        
        if current_word:
            word = ''.join(current_word)
            if len(word) > 2:
                words.append(word)
        
        return words

def sparse_cosine_similarity(vec1: Dict[int, float], vec2: Dict[int, float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªç¨€ç–å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if not vec1 or not vec2:
        return 0.0
    
    # è®¡ç®—ç‚¹ç§¯
    dot_product = 0.0
    for idx, val1 in vec1.items():
        if idx in vec2:
            dot_product += val1 * vec2[idx]
    
    # è®¡ç®—å‘é‡æ¨¡é•¿
    norm1 = math.sqrt(sum(val * val for val in vec1.values()))
    norm2 = math.sqrt(sum(val * val for val in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

class VectorRetrieval:
    def __init__(self, documents: Dict[str, Dict], cache_dir: str = "Meetup/cache"):
        self.documents = documents
        self.tfidf = OptimizedTFIDF(cache_dir=cache_dir)
        self.processing_times: Dict[str, float] = {}
        self.cache_dir = cache_dir
        self.doc_ids = []  # æ–‡æ¡£IDåˆ—è¡¨ï¼Œç´¢å¼•å¯¹åº”doc_vectorsä¸­çš„ç´¢å¼•
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
    
    def build_tfidf_vectors(self, use_cache: bool = True, max_features: int = 50000):
        """æž„å»ºTF-IDFå‘é‡"""
        start_time = time.time()
        
        # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬å’ŒIDæ˜ å°„
        doc_texts = []
        self.doc_ids = []
        
        # ä½¿ç”¨ç¨³å®šçš„æ–‡æ¡£é¡ºåºï¼Œç¡®ä¿ç¼“å­˜é”®ä¸Žç»“æžœå¯å¤çŽ°
        for doc_id in sorted(self.documents.keys()):
            doc_texts.append(self.documents[doc_id]['content'])
            self.doc_ids.append(doc_id)
        
        # ä½¿ç”¨ä¼˜åŒ–çš„TF-IDF
        self.doc_vectors = self.tfidf.fit_transform(doc_texts, use_cache=use_cache, max_features=max_features)
        
        end_time = time.time()
        self.processing_times['tfidf_building'] = end_time - start_time
        
        print(f"âœ… TF-IDF å‘é‡æž„å»ºå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.tfidf.vocab)}ï¼Œè€—æ—¶ {end_time - start_time:.2f}ç§’")
    
    def search(self, query: str, top_k: int = 10, use_cache: bool = True, max_features: int = 50000) -> Tuple[List[Tuple[str, float]], float]:
        """åŸºäºŽå‘é‡ç©ºé—´æ¨¡åž‹çš„æ£€ç´¢ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        start_time = time.time()
        
        if not hasattr(self, 'doc_vectors'):
            self.build_tfidf_vectors(use_cache=use_cache, max_features=max_features)
        
        # è½¬æ¢æŸ¥è¯¢ä¸ºç¨€ç–å‘é‡
        query_vectors = self.tfidf.transform([query])
        if not query_vectors:
            return [], 0.0
        
        query_vector = query_vectors[0]
        
        # å¦‚æžœæŸ¥è¯¢å‘é‡ä¸ºç©ºï¼Œè¿”å›žç©ºç»“æžœ
        if not query_vector:
            return [], 0.0
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        total_docs = len(self.doc_vectors)
        
        print(f"ðŸ” è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå…± {total_docs} ä¸ªæ–‡æ¡£...")
        
        # ä½¿ç”¨æ‰¹é‡å¤„ç†å‡å°‘å†…å­˜ä½¿ç”¨
        batch_size = 10000
        num_batches = (total_docs + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, total_docs)
            
            batch_progress = tqdm(
                range(start_idx, end_idx),
                desc=f"ðŸ” æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}",
                total=end_idx - start_idx,
                unit="æ–‡æ¡£",
                ncols=100
            )
            
            for doc_id in batch_progress:
                doc_vector = self.doc_vectors.get(doc_id, {})
                similarity = sparse_cosine_similarity(query_vector, doc_vector)
                if similarity > 0:  # åªä¿ç•™æœ‰ç›¸ä¼¼åº¦çš„æ–‡æ¡£
                    similarities.append((self.doc_ids[doc_id], similarity))
            
            batch_progress.close()
        
        # æŽ’åºå¹¶è¿”å›žtop-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = similarities[:top_k]
        
        end_time = time.time()
        search_time = end_time - start_time
        
        # è®°å½•æœç´¢æ—¶é—´
        if 'search_times' not in self.processing_times:
            self.processing_times['search_times'] = []
        self.processing_times['search_times'].append(search_time)
        
        return results, search_time
    
    def get_processing_times(self) -> Dict[str, float]:
        return self.processing_times