from typing import Dict, List, Set, Tuple
from collections import defaultdict
import math
import json
import time
import os
import hashlib
from tqdm import tqdm

class InvertedIndex:
    def __init__(self, cache_dir: str = "Meetup/cache"):
        self.index = defaultdict(dict)  # term -> {doc_id: [positions]}
        self.doc_lengths = {}  # æ–‡æ¡£é•¿åº¦ï¼ˆè¯é¡¹æ•°é‡ï¼‰
        self.doc_count = 0
        self.term_freq = defaultdict(dict)  # term -> {doc_id: frequency}
        self.processing_times = {}
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, normalized_docs: Dict[str, List[str]]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŸºäºè§„èŒƒåŒ–æ–‡æ¡£å†…å®¹"""
        # ä½¿ç”¨æ–‡æ¡£IDå’Œè§„èŒƒåŒ–è¯é¡¹ç”Ÿæˆå“ˆå¸Œ
        content = "".join([doc_id + "".join(terms) for doc_id, terms in normalized_docs.items()])
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cache_file(self, normalized_docs: Dict[str, List[str]]) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self._get_cache_key(normalized_docs)
        return os.path.join(self.cache_dir, f"index_{cache_key}.json")
    
    def load_index_from_cache(self, normalized_docs: Dict[str, List[str]]) -> bool:
        """ä»ç¼“å­˜åŠ è½½ç´¢å¼•"""
        cache_file = self._get_cache_file(normalized_docs)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                
                self.index = defaultdict(dict, index_data['index'])
                self.doc_lengths = index_data['doc_lengths']
                self.doc_count = index_data['doc_count']
                self.term_freq = defaultdict(dict, index_data['term_freq'])
                
                print(f"âœ… ä»ç¼“å­˜åŠ è½½äº†åŒ…å« {len(self.index)} ä¸ªè¯é¡¹çš„ç´¢å¼•")
                return True
            except Exception as e:
                print(f"âŒ åŠ è½½ç´¢å¼•ç¼“å­˜å¤±è´¥: {e}")
        
        return False
    
    def save_index_to_cache(self, normalized_docs: Dict[str, List[str]]):
        """ä¿å­˜ç´¢å¼•åˆ°ç¼“å­˜"""
        cache_file = self._get_cache_file(normalized_docs)
        try:
            index_data = {
                'metadata': {
                    'document_count': self.doc_count,
                    'term_count': len(self.index),
                    'timestamp': time.time()
                },
                'index': dict(self.index),
                'doc_lengths': self.doc_lengths,
                'doc_count': self.doc_count,
                'term_freq': dict(self.term_freq)
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•ç¼“å­˜å¤±è´¥: {e}")
    
    def build_index(self, normalized_docs: Dict[str, List[str]], use_cache: bool = True):
        """æ„å»ºå€’æ’ç´¢å¼•"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and self.load_index_from_cache(normalized_docs):
            return
            
        start_time = time.time()
        self.doc_count = len(normalized_docs)
        print(f"ğŸ“Š æ„å»ºå€’æ’ç´¢å¼•ï¼Œå…± {self.doc_count} ä¸ªæ–‡æ¡£")
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        progress_bar = tqdm(
            normalized_docs.items(),
            desc="ğŸ“Š æ„å»ºç´¢å¼•",
            total=self.doc_count,
            unit="æ–‡æ¡£",
            ncols=100,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
        
        for doc_id, terms in progress_bar:
            self.doc_lengths[doc_id] = len(terms)
            
            # è®°å½•è¯é¡¹ä½ç½®å’Œé¢‘ç‡
            term_freq_in_doc = defaultdict(int)
            for position, term in enumerate(terms):
                if doc_id not in self.index[term]:
                    self.index[term][doc_id] = []
                self.index[term][doc_id].append(position)
                term_freq_in_doc[term] += 1
            
            # æ›´æ–°è¯é¡¹é¢‘ç‡
            for term, freq in term_freq_in_doc.items():
                self.term_freq[term][doc_id] = freq
        
        # å…³é—­è¿›åº¦æ¡
        progress_bar.close()
        
        end_time = time.time()
        self.processing_times['index_building'] = end_time - start_time
        
        print(f"âœ… å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.index)} ä¸ªè¯é¡¹ï¼Œè€—æ—¶ {end_time - start_time:.2f}ç§’")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self.save_index_to_cache(normalized_docs)
    
    def get_term_frequency(self, term: str, doc_id: str) -> int:
        """è·å–è¯é¡¹åœ¨æ–‡æ¡£ä¸­çš„é¢‘ç‡"""
        return self.term_freq.get(term, {}).get(doc_id, 0)
    
    def get_document_frequency(self, term: str) -> int:
        """è·å–åŒ…å«è¯é¡¹çš„æ–‡æ¡£æ•°é‡"""
        return len(self.index.get(term, {}))
    
    def get_inverse_document_frequency(self, term: str) -> float:
        """è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡"""
        df = self.get_document_frequency(term)
        if df == 0:
            return 0
        return math.log(self.doc_count / df)
    
    def save_index(self, filepath: str):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        start_time = time.time()
        print("ğŸ’¾ ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶...", end=" ")
        
        index_data = {
            'index': dict(self.index),
            'doc_lengths': self.doc_lengths,
            'doc_count': self.doc_count,
            'term_freq': dict(self.term_freq)
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        end_time = time.time()
        self.processing_times['index_saving'] = end_time - start_time
        
        print("âœ…")
    
    def load_index(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        start_time = time.time()
        
        with open(filepath, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        self.index = defaultdict(dict, index_data['index'])
        self.doc_lengths = index_data['doc_lengths']
        self.doc_count = index_data['doc_count']
        self.term_freq = defaultdict(dict, index_data['term_freq'])
        
        end_time = time.time()
        self.processing_times['index_loading'] = end_time - start_time
    
    def get_processing_times(self) -> Dict:
        """è·å–å¤„ç†æ—¶é—´ç»Ÿè®¡"""
        return self.processing_times