import xml.etree.ElementTree as ET
import os
import glob
from typing import List, Dict
import time
import json
import hashlib
from tqdm import tqdm

class DataProcessor:
    def __init__(self, data_path: str, max_files: int = None, cache_dir: str = "Meetup/cache"):
        self.data_path = data_path
        self.documents = {}  # doc_id -> document_content
        self.max_files = max_files
        self.processing_times = {}
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
    def _get_cache_key(self) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŸºäºæ•°æ®è·¯å¾„å’Œæ–‡ä»¶æ•°é‡é™åˆ¶"""
        key_data = f"{self.data_path}_{self.max_files}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_cache_file(self) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self._get_cache_key()
        return os.path.join(self.cache_dir, f"documents_{cache_key}.json")
    
    def load_documents_from_cache(self) -> bool:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£"""
        cache_file = self._get_cache_file()
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'documents' in data and 'metadata' in data:
                    self.documents = data['documents']
                    print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")
                    return True
            except Exception as e:
                print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        return False
    
    def save_documents_to_cache(self):
        """ä¿å­˜æ–‡æ¡£åˆ°ç¼“å­˜"""
        cache_file = self._get_cache_file()
        try:
            data = {
                'metadata': {
                    'data_path': self.data_path,
                    'max_files': self.max_files,
                    'document_count': len(self.documents),
                    'timestamp': time.time()
                },
                'documents': self.documents
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        
    def parse_event_files(self, use_cache: bool = True):
        """è§£æEvent XMLæ–‡ä»¶"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and self.load_documents_from_cache():
            return
            
        print(f"ğŸ“‚ è§£æç›®å½•: {self.data_path}")
        start_time = time.time()
        
        # è·å–æ‰€æœ‰XMLæ–‡ä»¶
        xml_files = []
        for root, dirs, files in os.walk(self.data_path):
            for file in files:
                if file.endswith('.xml'):
                    xml_files.append(os.path.join(root, file))
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(xml_files)} ä¸ª XML æ–‡ä»¶")
        
        if self.max_files and self.max_files > 0:
            xml_files = xml_files[:self.max_files]
            print(f"ğŸ”¢ é™åˆ¶è§£æå‰ {self.max_files} ä¸ªæ–‡ä»¶")
        else:
            print("ğŸ”¢ å¤„ç†æ‰€æœ‰æ–‡ä»¶")
        
        processed_count = 0
        error_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        progress_bar = tqdm(
            xml_files,
            desc="ğŸ“‚ è§£æXMLæ–‡ä»¶",
            unit="æ–‡ä»¶",
            ncols=100,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
        
        for file_path in progress_bar:
            try:
                # è§£æXMLæ–‡ä»¶
                tree = ET.parse(file_path)
                root = tree.getroot()
                
                # æå–äº‹ä»¶ä¿¡æ¯
                event_id = self._get_text(root, 'id')
                event_name = self._get_text(root, 'name')
                description = self._get_text(root, 'description')
                
                # å°è¯•è·å–groupä¿¡æ¯
                group_elem = root.find('group')
                group_name = self._get_text(group_elem, 'name') if group_elem is not None else ""
                
                # åˆå¹¶ä¸ºæ–‡æ¡£å†…å®¹
                doc_content = f"{event_name} {description} {group_name}"
                
                if event_id and doc_content.strip():
                    self.documents[event_id] = {
                        'content': doc_content,
                        'name': event_name,
                        'group': group_name,
                        'file_path': file_path
                    }
                    processed_count += 1
                    
            except ET.ParseError as e:
                error_count += 1
            except Exception as e:
                error_count += 1
        
        # å…³é—­è¿›åº¦æ¡
        progress_bar.close()
        
        end_time = time.time()
        self.processing_times['data_parsing'] = end_time - start_time
        
        print(f"âœ… æ•°æ®è§£æå®Œæˆ: æˆåŠŸ {processed_count}, é”™è¯¯ {error_count}, è€—æ—¶ {end_time - start_time:.2f}ç§’")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self.save_documents_to_cache()
                
    def _get_text(self, element, tag: str) -> str:
        """å®‰å…¨è·å–XMLå…ƒç´ æ–‡æœ¬"""
        if element is None:
            return ""
        elem = element.find(tag)
        return elem.text if elem is not None else ""
    
    def get_documents(self) -> Dict[str, Dict]:
        return self.documents
    
    def get_document_count(self) -> int:
        return len(self.documents)
    
    def get_processing_times(self) -> Dict:
        """è·å–å¤„ç†æ—¶é—´ç»Ÿè®¡"""
        return self.processing_times