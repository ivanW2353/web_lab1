import os
import time
import argparse
from tqdm import tqdm

def main():
    """ä¸»å‡½æ•°"""
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(
        description='ä¿¡æ¯æ£€ç´¢å®éªŒç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´æµç¨‹ï¼ˆæ„å»º+æ¼”ç¤ºæŸ¥è¯¢ï¼‰
  python main.py --max_files 10000
  
  # ä»…æ„å»ºç´¢å¼•
  python main.py --mode build --max_files 10000
  
  # å¸ƒå°”æ£€ç´¢ï¼ˆå•æ¬¡æŸ¥è¯¢ï¼‰
  python main.py --mode boolean --query "meeting and group"
  
  # å¸ƒå°”æ£€ç´¢ï¼ˆäº¤äº’æ¨¡å¼ï¼‰
  python main.py --mode boolean
  
  # å‘é‡æ£€ç´¢ï¼ˆå•æ¬¡æŸ¥è¯¢ï¼‰
  python main.py --mode vector --query "technology conference" --top_k 5
  
  # å‘é‡æ£€ç´¢ï¼ˆäº¤äº’æ¨¡å¼ï¼‰
  python main.py --mode vector
        """)
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', type=str, 
                       choices=['full', 'build', 'boolean', 'vector'],
                       default='full',
                       help='è¿è¡Œæ¨¡å¼: full=å®Œæ•´æ¼”ç¤º, build=ä»…æ„å»º, boolean=å¸ƒå°”æ£€ç´¢, vector=å‘é‡æ£€ç´¢')
    
    # æ•°æ®å¤„ç†å‚æ•°
    parser.add_argument('--max_files', type=int, default=10000, 
                       help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•°é‡ (é»˜è®¤: 10000, 0=å…¨éƒ¨)')
    parser.add_argument('--data_path', type=str, default="Meetup/All_Unpack",
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: Meetup/All_Unpack)')
    parser.add_argument('--cache_dir', type=str, default="Meetup/cache",
                       help='ç¼“å­˜ç›®å½• (é»˜è®¤: Meetup/cache)')
    parser.add_argument('--index_file', type=str, default="Meetup/inverted_index.json",
                       help='ç´¢å¼•æ–‡ä»¶è·¯å¾„ (é»˜è®¤: Meetup/inverted_index.json)')
    
    # æŸ¥è¯¢å‚æ•°
    parser.add_argument('--query', type=str, default=None,
                       help='æŸ¥è¯¢è¯­å¥ï¼ˆç”¨äº boolean/vector æ¨¡å¼ï¼‰')
    parser.add_argument('--top_k', type=int, default=10,
                       help='å‘é‡æ£€ç´¢è¿”å›çš„æ–‡æ¡£æ•°é‡ (é»˜è®¤: 10)')
    parser.add_argument('--max_features', type=int, default=30000,
                       help='TF-IDF ç‰¹å¾æ•°é‡ä¸Šé™ (é»˜è®¤: 30000)')
    
    args = parser.parse_args()
    
    # æ ¹æ®æ¨¡å¼åˆ†å‘åˆ°ä¸åŒå‡½æ•°
    if args.mode == 'boolean':
        return run_boolean_search(args)
    elif args.mode == 'vector':
        return run_vector_search(args)
    elif args.mode == 'build':
        return run_build_only(args)
    else:  # full
        return run_full_demo(args)


def run_boolean_search(args):
    """å¸ƒå°”æ£€ç´¢æ¨¡å¼"""
    print("=== å¸ƒå°”æ£€ç´¢æ¨¡å¼ ===\n")
    
    if not os.path.exists(args.index_file):
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {args.index_file}")
        print(f"è¯·å…ˆè¿è¡Œæ„å»º: python main.py --mode build")
        return 1
    
    from inverted_index import InvertedIndex
    from boolean_retrieval import BooleanRetrieval
    
    # åŠ è½½ç´¢å¼•
    print(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {args.index_file}")
    inv = InvertedIndex(cache_dir=args.cache_dir)
    inv.load_index(args.index_file)
    print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ ({len(inv.index)} ä¸ªè¯é¡¹)\n")
    
    searcher = BooleanRetrieval(inv)
    
    # åŠ è½½æ–‡æ¡£ä¿¡æ¯ï¼ˆç”¨äºæ˜¾ç¤ºåç§°ï¼‰
    from data_processor import DataProcessor
    max_files = args.max_files if args.max_files > 0 else None
    processor = DataProcessor(data_path=args.data_path, max_files=max_files, cache_dir=args.cache_dir)
    documents = {}
    if processor.load_documents_from_cache():
        documents = processor.get_documents()
    
    def do_query(query: str):
        results = searcher.search(query)
        print(f"ğŸ” æŸ¥è¯¢: {query}")
        print(f"âœ… å‘½ä¸­æ–‡æ¡£æ•°: {len(results)}")
        if results:
            print("ç¤ºä¾‹å‰ 10 ä¸ªç»“æœ:")
            for i, doc_id in enumerate(list(results)[:10], 1):
                if documents and doc_id in documents:
                    name = documents[doc_id]['name']
                    if len(name) > 50:
                        name = name[:50] + '...'
                    print(f"  {i}. [{doc_id}] {name}")
                else:
                    print(f"  {i}. {doc_id}")
        print()
    
    # å•æ¬¡æŸ¥è¯¢æˆ–äº¤äº’æ¨¡å¼
    if args.query:
        do_query(args.query)
        return 0
    
    # äº¤äº’æ¨¡å¼
    print("ğŸ’¡ äº¤äº’æ¨¡å¼ - è¾“å…¥æŸ¥è¯¢è¯­å¥ï¼Œæˆ–è¾“å…¥ 'exit' é€€å‡º")
    print("æ”¯æŒ: å•è¯æŸ¥è¯¢, 'term1 and term2', 'term1 or term2', 'not term'\n")
    
    while True:
        try:
            query = input('query> ').strip()
        except (EOFError, KeyboardInterrupt):
            print('\nğŸ‘‹ å†è§!')
            break
        
        if not query or query.lower() in {'exit', 'quit', ':q'}:
            break
        
        do_query(query)
    
    return 0


def run_vector_search(args):
    """å‘é‡æ£€ç´¢æ¨¡å¼"""
    print("=== å‘é‡æ£€ç´¢æ¨¡å¼ ===\n")
    
    if not os.path.exists(args.data_path):
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
        return 1
    
    from nltk_downloader import NLTKDataDownloader
    from data_processor import DataProcessor
    from vector_retrieval import VectorRetrieval
    
    # å‡†å¤‡ NLTKï¼ˆå¯é€‰ï¼‰
    try:
        NLTKDataDownloader.download_required_data()
    except Exception:
        pass
    
    # åŠ è½½æ–‡æ¡£
    print("ğŸ“‚ åŠ è½½æ–‡æ¡£...")
    max_files = args.max_files if args.max_files > 0 else None
    processor = DataProcessor(data_path=args.data_path, max_files=max_files, cache_dir=args.cache_dir)
    
    if not processor.load_documents_from_cache():
        print("âš ï¸  ç¼“å­˜æœªæ‰¾åˆ°ï¼Œå¼€å§‹è§£ææ•°æ®...")
        processor.parse_event_files(use_cache=True)
    
    documents = processor.get_documents()
    if not documents:
        print('âŒ æ— æ³•åŠ è½½æ–‡æ¡£')
        return 2
    
    print(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆ ({len(documents)} ä¸ªæ–‡æ¡£)\n")
    
    # åˆå§‹åŒ–å‘é‡æ£€ç´¢
    vr = VectorRetrieval(documents, cache_dir=args.cache_dir)
    
    def do_query(query: str):
        results, cost = vr.search(query, top_k=args.top_k, use_cache=True, max_features=args.max_features)
        print(f"ğŸ” æŸ¥è¯¢: {query}")
        print(f"â±ï¸  è€—æ—¶: {cost:.4f}s  |  è¿”å›: {len(results)} æ¡")
        for i, (doc_id, score) in enumerate(results, 1):
            name = documents[doc_id]['name']
            if len(name) > 50:
                name = name[:50] + '...'
            print(f"  {i}. [ç›¸ä¼¼åº¦: {score:.4f}] [{doc_id}] {name}")
        print()
    
    # å•æ¬¡æŸ¥è¯¢æˆ–äº¤äº’æ¨¡å¼
    if args.query:
        do_query(args.query)
        return 0
    
    # äº¤äº’æ¨¡å¼
    print("ğŸ’¡ äº¤äº’æ¨¡å¼ - è¾“å…¥æŸ¥è¯¢è¯­å¥ï¼Œæˆ–è¾“å…¥ 'exit' é€€å‡º\n")
    
    while True:
        try:
            query = input('query> ').strip()
        except (EOFError, KeyboardInterrupt):
            print('\nğŸ‘‹ å†è§!')
            break
        
        if not query or query.lower() in {'exit', 'quit', ':q'}:
            break
        
        do_query(query)
    
    return 0


def run_build_only(args):
    """ä»…æ„å»ºæ¨¡å¼ï¼ˆä¸è¿è¡Œæ¼”ç¤ºæŸ¥è¯¢ï¼‰"""
    print("=== æ„å»ºæ¨¡å¼ ===\n")
    
    data_path = args.data_path
    cache_dir = args.cache_dir
    max_files = args.max_files
    
    print(f"é…ç½®å‚æ•°:")
    print(f"  â€¢ æœ€å¤§å¤„ç†æ–‡ä»¶æ•°: {max_files}")
    print(f"  â€¢ æ•°æ®è·¯å¾„: {data_path}")
    print(f"  â€¢ ç¼“å­˜ç›®å½•: {cache_dir}")
    print(f"  â€¢ ç´¢å¼•è¾“å‡º: {args.index_file}\n")
    
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®è·¯å¾„ '{data_path}' ä¸å­˜åœ¨!")
        print("è¯·ç¡®ä¿ Meetup/All_Unpack ç›®å½•å­˜åœ¨å¹¶åŒ…å« XML æ–‡ä»¶")
        return 1
    
    # é¦–å…ˆå°è¯•ä¸‹è½½ NLTK æ•°æ®
    try:
        from nltk_downloader import NLTKDataDownloader
        print("ğŸ” æ£€æŸ¥ NLTK æ•°æ®...")
        NLTKDataDownloader.download_required_data()
    except Exception as e:
        print(f"âš ï¸  NLTKæ•°æ®æ£€æŸ¥è·³è¿‡: {e}")
    
    print()
    
    # å¯¼å…¥æ¨¡å—
    from data_processor import DataProcessor
    from text_normalizer import TextNormalizer
    from inverted_index import InvertedIndex
    
    total_start_time = time.time()
    
    # 1. æ•°æ®è§£æ
    print("ğŸ“‚ æ­¥éª¤ 1: æ•°æ®è§£æ")
    print("â”€" * 40)
    
    processor = DataProcessor(data_path, max_files=max_files, cache_dir=cache_dir)
    processor.parse_event_files(use_cache=True)
    documents = processor.get_documents()
    
    if not documents:
        print("âŒ é”™è¯¯: æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡æ¡£!")
        return 2
    
    print(f"âœ… æˆåŠŸè§£æ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. æ–‡æœ¬è§„èŒƒåŒ–
    print("\nğŸ”¤ æ­¥éª¤ 2: æ–‡æœ¬è§„èŒƒåŒ–")
    print("â”€" * 40)
    
    normalizer = TextNormalizer(cache_dir=cache_dir)
    normalized_docs = normalizer.process_documents(documents, use_cache=True)
    
    print("âœ… æ–‡æœ¬è§„èŒƒåŒ–å®Œæˆ")
    
    # 3. æ„å»ºå€’æ’ç´¢å¼•
    print("\nğŸ“Š æ­¥éª¤ 3: æ„å»ºå€’æ’ç´¢å¼•")
    print("â”€" * 40)
    
    inverted_index = InvertedIndex(cache_dir=cache_dir)
    inverted_index.build_index(normalized_docs, use_cache=True)
    
    print(f"âœ… å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(inverted_index.index)} ä¸ªè¯é¡¹")
    
    # 4. ä¿å­˜ç´¢å¼•
    print("\nï¿½ æ­¥éª¤ 4: ä¿å­˜ç´¢å¼•")
    print("â”€" * 40)
    
    inverted_index.save_index(args.index_file)
    
    print(f"âœ… ç´¢å¼•å·²ä¿å­˜: {args.index_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ„å»ºå®Œæˆ!")
    print("=" * 60)
    
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
    print(f"å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    print(f"ç´¢å¼•è¯é¡¹æ•°: {len(inverted_index.index)}")
    
    # å„é˜¶æ®µè€—æ—¶
    print("\nâ±ï¸  å„é˜¶æ®µè€—æ—¶ç»Ÿè®¡:")
    
    processor_times = processor.get_processing_times()
    if 'data_parsing' in processor_times:
        print(f"  æ•°æ®è§£æ: {processor_times['data_parsing']:.2f} ç§’")
    
    normalizer_times = normalizer.get_processing_times()
    if 'text_normalization' in normalizer_times:
        print(f"  æ–‡æœ¬è§„èŒƒåŒ–: {normalizer_times['text_normalization']:.2f} ç§’")
    
    index_times = inverted_index.get_processing_times()
    if 'index_building' in index_times:
        print(f"  ç´¢å¼•æ„å»º: {index_times['index_building']:.2f} ç§’")
    
    if 'index_saving' in index_times:
        print(f"  ç´¢å¼•ä¿å­˜: {index_times['index_saving']:.2f} ç§’")
    
    print(f"\nğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
    print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --mode boolean/vector è¿›è¡Œæ£€ç´¢\n")
    
    return 0


def run_full_demo(args):
    """å®Œæ•´æ¼”ç¤ºæ¨¡å¼ï¼ˆæ„å»º+ç¤ºä¾‹æŸ¥è¯¢ï¼‰"""
    print("=== ä¿¡æ¯æ£€ç´¢å®éªŒç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º ===\n")
    
    data_path = args.data_path
    cache_dir = args.cache_dir
    max_files = args.max_files
    
    print(f"é…ç½®å‚æ•°:")
    print(f"  â€¢ æœ€å¤§å¤„ç†æ–‡ä»¶æ•°: {max_files}")
    print(f"  â€¢ æ•°æ®è·¯å¾„: {data_path}")
    print(f"  â€¢ ç¼“å­˜ç›®å½•: {cache_dir}\n")
    
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®è·¯å¾„ '{data_path}' ä¸å­˜åœ¨!")
        print("è¯·ç¡®ä¿ Meetup/All_Unpack ç›®å½•å­˜åœ¨å¹¶åŒ…å« XML æ–‡ä»¶")
        return 1
    
    # é¦–å…ˆå°è¯•ä¸‹è½½ NLTK æ•°æ®
    try:
        from nltk_downloader import NLTKDataDownloader
        print("ğŸ” æ£€æŸ¥ NLTK æ•°æ®...")
        success = NLTKDataDownloader.download_required_data()
        if success:
            print("âœ… NLTKæ•°æ®å·²å°±ç»ª")
        else:
            print("âš ï¸  NLTKæ•°æ®ä¸å®Œæ•´ï¼Œå°†ä½¿ç”¨å¢å¼ºçš„å†…ç½®å¤„ç†å™¨")
    except Exception as e:
        print(f"âš ï¸  NLTKæ•°æ®æ£€æŸ¥è·³è¿‡: {e}")
    
    # å¯¼å…¥æ¨¡å—
    from data_processor import DataProcessor
    from text_normalizer import TextNormalizer
    from inverted_index import InvertedIndex
    from boolean_retrieval import BooleanRetrieval
    
    # å°è¯•å¯¼å…¥å‘é‡æ£€ç´¢
    try:
        from vector_retrieval import VectorRetrieval
        VECTOR_AVAILABLE = True
    except ImportError as e:
        VECTOR_AVAILABLE = False
        print("â„¹ï¸  å‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤åŠŸèƒ½")
    
    total_start_time = time.time()
    
    # 1. æ•°æ®è§£æ
    print("\nğŸ“‚ æ­¥éª¤ 1: æ•°æ®è§£æ")
    print("â”€" * 40)
    
    processor = DataProcessor(data_path, max_files=max_files, cache_dir=cache_dir)
    processor.parse_event_files(use_cache=True)
    documents = processor.get_documents()
    
    if not documents:
        print("âŒ é”™è¯¯: æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡æ¡£!")
        return 2
    
    print(f"âœ… æˆåŠŸè§£æ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. æ–‡æœ¬è§„èŒƒåŒ–
    print("\nğŸ”¤ æ­¥éª¤ 2: æ–‡æœ¬è§„èŒƒåŒ–")
    print("â”€" * 40)
    
    normalizer = TextNormalizer(cache_dir=cache_dir)
    normalized_docs = normalizer.process_documents(documents, use_cache=True)
    
    print("âœ… æ–‡æœ¬è§„èŒƒåŒ–å®Œæˆ")
    
    # 3. æ„å»ºå€’æ’ç´¢å¼•
    print("\nğŸ“Š æ­¥éª¤ 3: æ„å»ºå€’æ’ç´¢å¼•")
    print("â”€" * 40)
    
    inverted_index = InvertedIndex(cache_dir=cache_dir)
    inverted_index.build_index(normalized_docs, use_cache=True)
    
    print(f"âœ… å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(inverted_index.index)} ä¸ªè¯é¡¹")
    
    # 4. å¸ƒå°”æ£€ç´¢æµ‹è¯•
    print("\nğŸ” æ­¥éª¤ 4: å¸ƒå°”æ£€ç´¢æµ‹è¯•")
    print("â”€" * 40)
    
    boolean_searcher = BooleanRetrieval(inverted_index)
    
    test_queries = [
        "party",
        "meeting and group", 
        "tech or computer",
        "not business"
    ]
    
    print("æ‰§è¡Œå¸ƒå°”æŸ¥è¯¢:")
    for query in test_queries:
        start_time = time.time()
        results = boolean_searcher.search(query)
        end_time = time.time()
        search_time = end_time - start_time
        print(f"  â€¢ '{query}' -> {len(results)} ä¸ªç»“æœ (è€—æ—¶: {search_time:.4f}ç§’)")
    
    print("âœ… å¸ƒå°”æ£€ç´¢æµ‹è¯•å®Œæˆ")
    
    # 5. å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    vector_searcher = None
    if VECTOR_AVAILABLE:
        print("\nğŸ“ˆ æ­¥éª¤ 5: å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢")
        print("â”€" * 40)
        
        vector_searcher = VectorRetrieval(documents, cache_dir=cache_dir)
        
        vector_queries = [
            "technology conference",
            "business meeting", 
            "social event party"
        ]
        
        print("æ‰§è¡Œå‘é‡æŸ¥è¯¢:")
        for query in vector_queries:
            print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
            
            try:
                # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡æ£€ç´¢ï¼Œé™åˆ¶ç‰¹å¾æ•°é‡
                results, search_time = vector_searcher.search(query, top_k=3, use_cache=True, max_features=30000)
                print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ (è€—æ—¶: {search_time:.4f}ç§’)")
                
                for i, (doc_id, score) in enumerate(results[:3], 1):
                    doc_name = documents[doc_id]['name']
                    if len(doc_name) > 50:
                        doc_name = doc_name[:50] + "..."
                    print(f"    {i}. [ç›¸ä¼¼åº¦: {score:.4f}] [{doc_id}] {doc_name}")
                    
            except ValueError as e:
                print(f"âŒ å‘é‡æ£€ç´¢é”™è¯¯: {e}")
                print("è·³è¿‡æ­¤æŸ¥è¯¢...")
            except MemoryError as e:
                print(f"âŒ å†…å­˜ä¸è¶³: {e}")
                print("å°è¯•è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾æ•°é‡...")
                # å°è¯•ä½¿ç”¨æ›´å°‘çš„ç‰¹å¾
                try:
                    results, search_time = vector_searcher.search(query, top_k=3, use_cache=True, max_features=10000)
                    print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ (è€—æ—¶: {search_time:.4f}ç§’)")
                    
                    for i, (doc_id, score) in enumerate(results[:3], 1):
                        doc_name = documents[doc_id]['name']
                        if len(doc_name) > 50:
                            doc_name = doc_name[:50] + "..."
                        print(f"    {i}. [ç›¸ä¼¼åº¦: {score:.4f}] [{doc_id}] {doc_name}")
                except:
                    print("âŒ å‘é‡æ£€ç´¢å¤±è´¥ï¼Œè·³è¿‡æ­¤æŸ¥è¯¢")
        
        print("\nâœ… å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢å®Œæˆ")
    else:
        print("\nğŸ“ˆ æ­¥éª¤ 5: å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢")
        print("â”€" * 40)
        print("â„¹ï¸  å‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
    
    # 6. ä¿å­˜ç´¢å¼•
    print("\nğŸ’¾ æ­¥éª¤ 6: ä¿å­˜ç´¢å¼•")
    print("â”€" * 40)
    
    inverted_index.save_index(args.index_file)
    
    print("âœ… ç´¢å¼•ä¿å­˜å®Œæˆ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å®éªŒå®Œæˆ!")
    print("=" * 60)
    
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
    print(f"å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    print(f"ç´¢å¼•è¯é¡¹æ•°: {len(inverted_index.index)}")
    
    # å„é˜¶æ®µè€—æ—¶
    print("\nâ±ï¸  å„é˜¶æ®µè€—æ—¶ç»Ÿè®¡:")
    
    # æ•°æ®è§£ææ—¶é—´
    processor_times = processor.get_processing_times()
    if 'data_parsing' in processor_times:
        print(f"  æ•°æ®è§£æ: {processor_times['data_parsing']:.2f} ç§’")
    
    # æ–‡æœ¬è§„èŒƒåŒ–æ—¶é—´
    normalizer_times = normalizer.get_processing_times()
    if 'text_normalization' in normalizer_times:
        print(f"  æ–‡æœ¬è§„èŒƒåŒ–: {normalizer_times['text_normalization']:.2f} ç§’")
    
    # ç´¢å¼•æ„å»ºæ—¶é—´
    index_times = inverted_index.get_processing_times()
    if 'index_building' in index_times:
        print(f"  ç´¢å¼•æ„å»º: {index_times['index_building']:.2f} ç§’")
    
    # ç´¢å¼•ä¿å­˜æ—¶é—´
    if 'index_saving' in index_times:
        print(f"  ç´¢å¼•ä¿å­˜: {index_times['index_saving']:.2f} ç§’")
    
    # å¸ƒå°”æ£€ç´¢æ—¶é—´ç»Ÿè®¡
    boolean_times = boolean_searcher.get_processing_times()
    if 'search_times' in boolean_times and boolean_times['search_times']:
        avg_bool_time = sum(boolean_times['search_times']) / len(boolean_times['search_times'])
        print(f"  å¹³å‡å¸ƒå°”æ£€ç´¢: {avg_bool_time:.4f} ç§’")
    
    # å‘é‡æ£€ç´¢æ—¶é—´ç»Ÿè®¡
    if vector_searcher and hasattr(vector_searcher, 'get_processing_times'):
        vector_times = vector_searcher.get_processing_times()
        if 'search_times' in vector_times and vector_times['search_times']:
            avg_vector_time = sum(vector_times['search_times']) / len(vector_times['search_times'])
            print(f"  å¹³å‡å‘é‡æ£€ç´¢: {avg_vector_time:.4f} ç§’")
        
        if 'tfidf_building' in vector_times:
            print(f"  TF-IDFæ„å»º: {vector_times['tfidf_building']:.2f} ç§’")
    
    # ç¼“å­˜ä½¿ç”¨æƒ…å†µ
    print(f"\nğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
    
    if os.path.exists(cache_dir):
        cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.json')]
        print(f"  ç¼“å­˜æ–‡ä»¶æ•°é‡: {len(cache_files)}")
    
    print(f"\nâœ¨ æ‰€æœ‰æ“ä½œå·²å®Œæˆï¼")
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  â€¢ å¸ƒå°”æ£€ç´¢: python main.py --mode boolean")
    print(f"  â€¢ å‘é‡æ£€ç´¢: python main.py --mode vector")
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main() or 0)