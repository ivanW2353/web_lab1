import os
import time
import argparse
from tqdm import tqdm

def main():
    """ä¸»å‡½æ•°"""
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='ä¿¡æ¯æ£€ç´¢å®éªŒç³»ç»Ÿ')
    parser.add_argument('--max_files', type=int, default=10000, 
                       help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•°é‡ (é»˜è®¤: 10000)')
    parser.add_argument('--data_path', type=str, default="Meetup/All_Unpack",
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: Meetup/All_Unpack)')
    parser.add_argument('--cache_dir', type=str, default="Meetup/cache",
                       help='ç¼“å­˜ç›®å½• (é»˜è®¤: Meetup/cache)')
    
    args = parser.parse_args()
    
    print("=== ä¿¡æ¯æ£€ç´¢å®éªŒç³»ç»Ÿå¯åŠ¨ ===\n")
    
    # ä½¿ç”¨å‚æ•°
    data_path = args.data_path
    cache_dir = args.cache_dir
    max_files = args.max_files
    
    print(f"é…ç½®å‚æ•°:")
    print(f"  â€¢ æœ€å¤§å¤„ç†æ–‡ä»¶æ•°: {max_files}")
    print(f"  â€¢ æ•°æ®è·¯å¾„: {data_path}")
    print(f"  â€¢ ç¼“å­˜ç›®å½•: {cache_dir}\n")
    
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®è·¯å¾„ '{data_path}' ä¸å­˜åœ¨!")
        print("è¯·ç¡®ä¿ Meetup/All_Unpack ç›®å½•å­˜åœ¨å¹¶åŒ…å« XML æ–‡ä»¶")
        return
    
    # é¦–å…ˆå°è¯•ä¸‹è½½ NLTK æ•°æ®
    try:
        from nltk_downloader import NLTKDataDownloader
        print("ğŸ” æ£€æŸ¥ NLTK æ•°æ®...")
        success = NLTKDataDownloader.download_required_data()
        if success:
            print("âœ… NLTKæ•°æ®æ£€æŸ¥å®Œæˆ")
        else:
            print("âš ï¸  NLTKæ•°æ®éƒ¨åˆ†ç¼ºå¤±ï¼Œä½¿ç”¨å†…ç½®å¤„ç†å™¨")
    except Exception as e:
        print(f"âš ï¸  NLTKæ•°æ®æ£€æŸ¥è·³è¿‡: {e}")
    
    # å¯¼å…¥æ¨¡å—
    from data_processor import DataProcessor
    from text_normalizer import TextNormalizer
    from inverted_index import InvertedIndex
    from boolean_retrieval import BooleanRetrieval
    
    # å°è¯•å¯¼å…¥å‘é‡æ£€ç´¢
    try:
        from vector_retrieval import VectorRetrieval
        VECTOR_AVAILABLE = True
    except ImportError as e:
        VECTOR_AVAILABLE = False
        print("â„¹ï¸  å‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤åŠŸèƒ½")
    
    total_start_time = time.time()
    
    # 1. æ•°æ®è§£æ
    print("\nğŸ“‚ æ­¥éª¤ 1: æ•°æ®è§£æ")
    print("â”€" * 40)
    
    processor = DataProcessor(data_path, max_files=max_files, cache_dir=cache_dir)
    processor.parse_event_files(use_cache=True)
    documents = processor.get_documents()
    
    if not documents:
        print("âŒ é”™è¯¯: æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡æ¡£!")
        return
    
    print(f"âœ… æˆåŠŸè§£æ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. æ–‡æœ¬è§„èŒƒåŒ–
    print("\nğŸ”¤ æ­¥éª¤ 2: æ–‡æœ¬è§„èŒƒåŒ–")
    print("â”€" * 40)
    
    normalizer = TextNormalizer(cache_dir=cache_dir)
    normalized_docs = normalizer.process_documents(documents, use_cache=True)
    
    print("âœ… æ–‡æœ¬è§„èŒƒåŒ–å®Œæˆ")
    
    # 3. æ„å»ºå€’æ’ç´¢å¼•
    print("\nğŸ“Š æ­¥éª¤ 3: æ„å»ºå€’æ’ç´¢å¼•")
    print("â”€" * 40)
    
    inverted_index = InvertedIndex(cache_dir=cache_dir)
    inverted_index.build_index(normalized_docs, use_cache=True)
    
    print(f"âœ… å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(inverted_index.index)} ä¸ªè¯é¡¹")
    
    # 4. å¸ƒå°”æ£€ç´¢æµ‹è¯•
    print("\nğŸ” æ­¥éª¤ 4: å¸ƒå°”æ£€ç´¢æµ‹è¯•")
    print("â”€" * 40)
    
    boolean_searcher = BooleanRetrieval(inverted_index)
    
    test_queries = [
        "party",
        "meeting and group", 
        "tech or computer",
        "not business"
    ]
    
    print("æ‰§è¡Œå¸ƒå°”æŸ¥è¯¢:")
    for query in test_queries:
        start_time = time.time()
        results = boolean_searcher.search(query)
        end_time = time.time()
        search_time = end_time - start_time
        print(f"  â€¢ '{query}' -> {len(results)} ä¸ªç»“æœ (è€—æ—¶: {search_time:.4f}ç§’)")
    
    print("âœ… å¸ƒå°”æ£€ç´¢æµ‹è¯•å®Œæˆ")
    
    # 5. å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    vector_searcher = None
    if VECTOR_AVAILABLE:
        print("\nğŸ“ˆ æ­¥éª¤ 5: å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢")
        print("â”€" * 40)
        
        vector_searcher = VectorRetrieval(documents, cache_dir=cache_dir)
        
        vector_queries = [
            "technology conference",
            "business meeting", 
            "social event party"
        ]
        
        print("æ‰§è¡Œå‘é‡æŸ¥è¯¢:")
        for query in vector_queries:
            print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
            
            try:
                # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡æ£€ç´¢ï¼Œé™åˆ¶ç‰¹å¾æ•°é‡
                results, search_time = vector_searcher.search(query, top_k=3, use_cache=True, max_features=30000)
                print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ (è€—æ—¶: {search_time:.4f}ç§’)")
                
                for i, (doc_id, score) in enumerate(results[:3], 1):
                    doc_name = documents[doc_id]['name']
                    if len(doc_name) > 50:
                        doc_name = doc_name[:50] + "..."
                    print(f"    {i}. [ç›¸ä¼¼åº¦: {score:.4f}] {doc_name}")
                    
            except ValueError as e:
                print(f"âŒ å‘é‡æ£€ç´¢é”™è¯¯: {e}")
                print("è·³è¿‡æ­¤æŸ¥è¯¢...")
            except MemoryError as e:
                print(f"âŒ å†…å­˜ä¸è¶³: {e}")
                print("å°è¯•è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾æ•°é‡...")
                # å°è¯•ä½¿ç”¨æ›´å°‘çš„ç‰¹å¾
                try:
                    results, search_time = vector_searcher.search(query, top_k=3, use_cache=True, max_features=10000)
                    print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ (è€—æ—¶: {search_time:.4f}ç§’)")
                    
                    for i, (doc_id, score) in enumerate(results[:3], 1):
                        doc_name = documents[doc_id]['name']
                        if len(doc_name) > 50:
                            doc_name = doc_name[:50] + "..."
                        print(f"    {i}. [ç›¸ä¼¼åº¦: {score:.4f}] {doc_name}")
                except:
                    print("âŒ å‘é‡æ£€ç´¢å¤±è´¥ï¼Œè·³è¿‡æ­¤æŸ¥è¯¢")
        
        print("âœ… å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢å®Œæˆ")
    else:
        print("\nğŸ“ˆ æ­¥éª¤ 5: å‘é‡ç©ºé—´æ¨¡å‹æ£€ç´¢")
        print("â”€" * 40)
        print("â„¹ï¸  å‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
    
    # 6. ä¿å­˜ç´¢å¼•
    print("\nğŸ’¾ æ­¥éª¤ 6: ä¿å­˜ç´¢å¼•")
    print("â”€" * 40)
    
    inverted_index.save_index("Meetup/inverted_index.json")
    
    print("âœ… ç´¢å¼•ä¿å­˜å®Œæˆ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å®éªŒå®Œæˆ!")
    print("=" * 60)
    
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
    print(f"å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    print(f"ç´¢å¼•è¯é¡¹æ•°: {len(inverted_index.index)}")
    
    # å„é˜¶æ®µè€—æ—¶
    print("\nâ±ï¸  å„é˜¶æ®µè€—æ—¶ç»Ÿè®¡:")
    
    # æ•°æ®è§£ææ—¶é—´
    processor_times = processor.get_processing_times()
    if 'data_parsing' in processor_times:
        print(f"  æ•°æ®è§£æ: {processor_times['data_parsing']:.2f} ç§’")
    
    # æ–‡æœ¬è§„èŒƒåŒ–æ—¶é—´
    normalizer_times = normalizer.get_processing_times()
    if 'text_normalization' in normalizer_times:
        print(f"  æ–‡æœ¬è§„èŒƒåŒ–: {normalizer_times['text_normalization']:.2f} ç§’")
    
    # ç´¢å¼•æ„å»ºæ—¶é—´
    index_times = inverted_index.get_processing_times()
    if 'index_building' in index_times:
        print(f"  ç´¢å¼•æ„å»º: {index_times['index_building']:.2f} ç§’")
    
    # ç´¢å¼•ä¿å­˜æ—¶é—´
    if 'index_saving' in index_times:
        print(f"  ç´¢å¼•ä¿å­˜: {index_times['index_saving']:.2f} ç§’")
    
    # å¸ƒå°”æ£€ç´¢æ—¶é—´ç»Ÿè®¡
    boolean_times = boolean_searcher.get_processing_times()
    if 'search_times' in boolean_times and boolean_times['search_times']:
        avg_bool_time = sum(boolean_times['search_times']) / len(boolean_times['search_times'])
        print(f"  å¹³å‡å¸ƒå°”æ£€ç´¢: {avg_bool_time:.4f} ç§’")
    
    # å‘é‡æ£€ç´¢æ—¶é—´ç»Ÿè®¡
    if vector_searcher and hasattr(vector_searcher, 'get_processing_times'):
        vector_times = vector_searcher.get_processing_times()
        if 'search_times' in vector_times and vector_times['search_times']:
            avg_vector_time = sum(vector_times['search_times']) / len(vector_times['search_times'])
            print(f"  å¹³å‡å‘é‡æ£€ç´¢: {avg_vector_time:.4f} ç§’")
        
        if 'tfidf_building' in vector_times:
            print(f"  TF-IDFæ„å»º: {vector_times['tfidf_building']:.2f} ç§’")
    
    # ç¼“å­˜ä½¿ç”¨æƒ…å†µ
    print(f"\nğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
    
    if os.path.exists(cache_dir):
        cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.json')]
        print(f"  ç¼“å­˜æ–‡ä»¶æ•°é‡: {len(cache_files)}")
    
    print(f"\nâœ¨ æ‰€æœ‰æ“ä½œå·²å®Œæˆï¼")

if __name__ == "__main__":
    main()