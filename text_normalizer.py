import re
import time
import sys
import os
from typing import List, Dict
import json
import hashlib
from tqdm import tqdm

class TextNormalizer:
    """ç»Ÿä¸€çš„æ–‡æœ¬è§„èŒƒåŒ–å™¨ï¼Œè‡ªåŠ¨å¤„ç† NLTK å¯ç”¨æ€§"""
    
    def __init__(self, cache_dir: str = "Meetup/cache"):
        self.processing_times = {}
        self.nltk_available = False
        self.stop_words = set()
        self.stemmer = None
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†ç»„ä»¶
        self._initialize_processor()
    
    def _initialize_processor(self):
        """åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨"""
        # è®¾ç½®åœç”¨è¯
        self._setup_comprehensive_stopwords()
        
        # æ£€æŸ¥NLTKæ˜¯å¦å¯ç”¨
        self._check_nltk_availability()
        
        if self.nltk_available:
            print("ğŸ”¤ ä½¿ç”¨ NLTK æ–‡æœ¬å¤„ç†å™¨")
        else:
            print("ğŸ”¤ ä½¿ç”¨å¢å¼ºå†…ç½®æ–‡æœ¬å¤„ç†å™¨")
    
    def _check_nltk_availability(self):
        """æ£€æŸ¥NLTKæ˜¯å¦å¯ç”¨"""
        try:
            import nltk
            from nltk.tokenize import word_tokenize
            from nltk.corpus import stopwords
            from nltk.stem import PorterStemmer
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å¯ç”¨
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
            
            # åˆå§‹åŒ–NLTKç»„ä»¶
            self.stop_words = set(stopwords.words('english'))
            self.stemmer = PorterStemmer()
            self.nltk_available = True
            
        except (LookupError, ImportError, OSError):
            self.nltk_available = False
    
    def _setup_comprehensive_stopwords(self):
        """è®¾ç½®å…¨é¢çš„åœç”¨è¯åˆ—è¡¨"""
        self.stop_words = {
            'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 
            'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 
            'after', 'above', 'below', 'between', 'among', 'is', 'are', 'was', 'were', 
            'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 
            'would', 'could', 'should', 'may', 'might', 'must', 'can', 'it', 'its', 
            'they', 'them', 'their', 'what', 'which', 'who', 'whom', 'this', 'that', 
            'these', 'those', 'am', 'is', 'are', 'was', 'were', 'been', 'being', 
            'have', 'has', 'had', 'do', 'does', 'did', 'shall', 'will', 'would', 
            'may', 'might', 'must', 'can', 'could', 'should', 'ought', 'i', 'me', 
            'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 
            'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 
            'themselves'
        }
    
    def _get_cache_key(self, documents: Dict[str, Dict]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹"""
        content = "".join([doc_id + doc_info['content'] for doc_id, doc_info in documents.items()])
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cache_file(self, documents: Dict[str, Dict]) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self._get_cache_key(documents)
        return os.path.join(self.cache_dir, f"normalized_{cache_key}.json")
    
    def load_normalized_docs_from_cache(self, documents: Dict[str, Dict]) -> Dict[str, List[str]]:
        """ä»ç¼“å­˜åŠ è½½è§„èŒƒåŒ–æ–‡æ¡£"""
        cache_file = self._get_cache_file(documents)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'normalized_docs' in data and 'metadata' in data:
                    print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(data['normalized_docs'])} ä¸ªè§„èŒƒåŒ–æ–‡æ¡£")
                    return data['normalized_docs']
            except Exception as e:
                print(f"âŒ åŠ è½½è§„èŒƒåŒ–ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def save_normalized_docs_to_cache(self, normalized_docs: Dict[str, List[str]], documents: Dict[str, Dict]):
        """ä¿å­˜è§„èŒƒåŒ–æ–‡æ¡£åˆ°ç¼“å­˜"""
        cache_file = self._get_cache_file(documents)
        try:
            data = {
                'metadata': {
                    'document_count': len(normalized_docs),
                    'timestamp': time.time(),
                    'processor_type': 'NLTK' if self.nltk_available else 'å†…ç½®'
                },
                'normalized_docs': normalized_docs
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è§„èŒƒåŒ–ç¼“å­˜å¤±è´¥: {e}")
    
    def normalize_text(self, text: str) -> List[str]:
        """æ–‡æœ¬è§„èŒƒåŒ–å¤„ç†æµç¨‹"""
        if not text or not text.strip():
            return []
        
        try:
            # 1. è½¬æ¢ä¸ºå°å†™
            text = text.lower().strip()
            
            # 2. æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤æ ‡ç‚¹ã€æ•°å­—ï¼Œä¿ç•™è¿å­—ç¬¦å’ŒåŸºæœ¬ç¬¦å·
            text = re.sub(r'[^a-zA-Z\s\-]', ' ', text)
            
            # 3. åˆ†è¯
            if self.nltk_available:
                tokens = self._nltk_tokenize(text)
            else:
                tokens = self._enhanced_tokenize(text)
            
            # 4. è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            tokens = [token for token in tokens 
                     if token not in self.stop_words and len(token) > 2]
            
            # 5. è¯å¹²æå–
            if self.nltk_available and self.stemmer:
                tokens = [self.stemmer.stem(token) for token in tokens]
            else:
                tokens = [self._enhanced_stem(token) for token in tokens]
            
            return tokens
            
        except Exception:
            # å¦‚æœä»»ä½•æ­¥éª¤å¤±è´¥ï¼Œä½¿ç”¨æœ€ç®€åŒ–çš„åˆ†è¯
            return self._minimal_tokenize(text)
    
    def _nltk_tokenize(self, text: str) -> List[str]:
        """ä½¿ç”¨ NLTK åˆ†è¯"""
        try:
            from nltk.tokenize import word_tokenize
            return word_tokenize(text)
        except Exception:
            return self._enhanced_tokenize(text)
    
    def _enhanced_tokenize(self, text: str) -> List[str]:
        """å¢å¼ºçš„åˆ†è¯å™¨"""
        # å¤„ç†å¸¸è§çš„ç¼©å†™å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r"n't\b", " not", text)
        text = re.sub(r"'s\b", "", text)
        text = re.sub(r"'re\b", " are", text)
        text = re.sub(r"'ve\b", " have", text)
        text = re.sub(r"'ll\b", " will", text)
        text = re.sub(r"'d\b", " would", text)
        text = re.sub(r"'m\b", " am", text)
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œåˆ†è¯
        tokens = re.findall(r'\b[a-zA-Z][a-zA-Z\-]+\b', text)
        
        # å¤„ç†è¿å­—ç¬¦å•è¯
        processed_tokens = []
        for token in tokens:
            if '-' in token:
                # åˆ†å‰²è¿å­—ç¬¦å•è¯ï¼Œä½†ä¿ç•™å¸¸è§çš„å¤åˆè¯
                parts = token.split('-')
                if len(parts) == 2 and len(parts[0]) > 2 and len(parts[1]) > 2:
                    # å¯¹äºå¸¸è§çš„å¤åˆè¯ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“å’Œéƒ¨åˆ†
                    processed_tokens.append(token)  # æ•´ä½“
                    processed_tokens.extend([part for part in parts if len(part) > 2])  # éƒ¨åˆ†
                else:
                    processed_tokens.extend([part for part in parts if len(part) > 2])
            else:
                processed_tokens.append(token)
        
        return processed_tokens
    
    def _minimal_tokenize(self, text: str) -> List[str]:
        """æœ€ç®€åŒ–çš„åˆ†è¯æ–¹æ¡ˆ"""
        if not text:
            return []
        
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s\-]', ' ', text)
        tokens = text.split()
        
        # å¤„ç†è¿å­—ç¬¦
        processed_tokens = []
        for token in tokens:
            if '-' in token:
                parts = [part for part in token.split('-') if len(part) > 2]
                processed_tokens.extend(parts)
            else:
                processed_tokens.append(token)
        
        tokens = [token for token in processed_tokens 
                 if token not in self.stop_words and len(token) > 2]
        
        tokens = [self._enhanced_stem(token) for token in tokens]
        
        return tokens
    
    def _enhanced_stem(self, word: str) -> str:
        """å¢å¼ºçš„è¯å¹²æå–"""
        if len(word) <= 3:
            return word
        
        # å¤„ç†å¤æ•°å½¢å¼
        if word.endswith('ies') and len(word) > 3:
            return word[:-3] + 'y'
        elif word.endswith('es') and len(word) > 2:
            base = word[:-2]
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šç»“å°¾
            if base.endswith(('s', 'x', 'z', 'ch', 'sh')):
                return base
            else:
                return word[:-1]
        elif word.endswith('s') and len(word) > 1 and not word.endswith('ss'):
            return word[:-1]
        
        # å¤„ç†åŠ¨è¯å½¢å¼
        if word.endswith('ing') and len(word) > 3:
            base = word[:-3]
            # åŒå†™è¾…éŸ³å­—æ¯è§„åˆ™
            if len(base) > 1 and base[-1] == base[-2] and base[-1] in 'bdgmnprt':
                return base[:-1]
            else:
                return base
        elif word.endswith('ed') and len(word) > 2:
            base = word[:-2]
            # åŒå†™è¾…éŸ³å­—æ¯è§„åˆ™
            if len(base) > 1 and base[-1] == base[-2] and base[-1] in 'bdgmnprt':
                return base[:-1]
            else:
                return base
        
        # å¤„ç†å‰¯è¯
        if word.endswith('ly') and len(word) > 2:
            return word[:-2]
        
        # å¤„ç†åè¯åç¼€
        if word.endswith('ment') and len(word) > 4:
            return word[:-4]
        elif word.endswith('ness') and len(word) > 4:
            return word[:-4]
        elif word.endswith('tion') and len(word) > 4:
            return word[:-4]
        
        return word
    
    def process_documents(self, documents: Dict[str, Dict], use_cache: bool = True) -> Dict[str, List[str]]:
        """å¤„ç†æ‰€æœ‰æ–‡æ¡£"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_docs = self.load_normalized_docs_from_cache(documents)
            if cached_docs is not None:
                return cached_docs
        
        start_time = time.time()
        normalized_docs = {}
        total_docs = len(documents)
        
        processor_type = "NLTK" if self.nltk_available else "å¢å¼ºå†…ç½®"
        print(f"ğŸ”¤ æ–‡æœ¬è§„èŒƒåŒ–å¤„ç† (ä½¿ç”¨{processor_type}å¤„ç†å™¨)")
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        progress_bar = tqdm(
            documents.items(),
            desc="ğŸ”¤ æ–‡æœ¬è§„èŒƒåŒ–",
            total=total_docs,
            unit="æ–‡æ¡£",
            ncols=100,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
        
        for doc_id, doc_info in progress_bar:
            content = doc_info['content']
            normalized_docs[doc_id] = self.normalize_text(content)
        
        # å…³é—­è¿›åº¦æ¡
        progress_bar.close()
        
        end_time = time.time()
        self.processing_times['text_normalization'] = end_time - start_time
        
        print(f"âœ… æ–‡æœ¬è§„èŒƒåŒ–å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f}ç§’")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self.save_normalized_docs_to_cache(normalized_docs, documents)
        
        return normalized_docs
    
    def get_processing_times(self) -> Dict:
        """è·å–å¤„ç†æ—¶é—´ç»Ÿè®¡"""
        return self.processing_times