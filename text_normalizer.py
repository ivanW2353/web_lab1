import re
import time
import sys
import os
from typing import List, Dict, Tuple
import json
import hashlib
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
from functools import partial


# ============ å…¨å±€è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰ ============

def _process_single_document(doc_item: Tuple[str, Dict]) -> Tuple[str, List[str]]:
    """
    å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
    å¿…é¡»æ˜¯é¡¶å±‚å‡½æ•°æ‰èƒ½è¢«pickleåºåˆ—åŒ–
    """
    doc_id, doc_info = doc_item
    content = doc_info['content']
    
    # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„è§„èŒƒåŒ–å™¨å®ä¾‹
    normalizer = _SimpleNormalizer()
    return (doc_id, normalizer.normalize(content))


class _SimpleNormalizer:
    """ç®€åŒ–çš„è§„èŒƒåŒ–å™¨ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼Œé¿å…NLTKåºåˆ—åŒ–é—®é¢˜ï¼‰"""
    
    def __init__(self):
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.pattern_clean = re.compile(r'[^a-zA-Z\s\-]')
        self.pattern_tokenize = re.compile(r'\b[a-zA-Z][a-zA-Z\-]+\b')
        
        # åœç”¨è¯é›†åˆ
        self.stop_words = frozenset([
            'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
            'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
            'after', 'above', 'below', 'between', 'among', 'is', 'are', 'was', 'were',
            'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
            'would', 'could', 'should', 'may', 'might', 'must', 'can', 'it', 'its',
            'they', 'them', 'their', 'what', 'which', 'who', 'whom', 'this', 'that',
            'these', 'those', 'am', 'shall', 'ought', 'i', 'me', 'my', 'myself', 'we',
            'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',
            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'itself',
            'themselves'
        ])
        
        # è¯å¹²æå–è¾…åŠ©é›†åˆ
        self.double_consonants = frozenset('bdgmnprt')
        self.suffix2_endings = frozenset(['ch', 'sh', 'ss'])
        self.suffix4_set = frozenset(['ment', 'ness', 'tion'])
    
    def normalize(self, text: str) -> List[str]:
        """å¿«é€Ÿè§„èŒƒåŒ–ï¼ˆä¸ä¾èµ–NLTKï¼‰"""
        if not text or not text.strip():
            return []
        
        # è½¬æ¢ä¸ºå°å†™å¹¶æ¸…ç†
        text = text.lower().strip()
        text = self.pattern_clean.sub(' ', text)
        
        # åˆ†è¯
        tokens = self.pattern_tokenize.findall(text)
        
        # è¿‡æ»¤å¹¶è¯å¹²æå–
        result = []
        for token in tokens:
            if token not in self.stop_words and len(token) > 2:
                result.append(self._stem(token))
        
        return result
    
    def _stem(self, word: str) -> str:
        """å¿«é€Ÿè¯å¹²æå–"""
        word_len = len(word)
        
        if word_len <= 3:
            return word
        
        # å¤„ç†å„ç§åç¼€
        if word_len > 3 and word[-3:] == 'ies':
            return word[:-3] + 'y'
        
        if word_len > 2 and word[-2:] == 'es':
            base = word[:-2]
            if len(base) >= 2 and base[-2:] in self.suffix2_endings:
                return base
            if len(base) >= 1 and base[-1] in 'sxz':
                return base
            return word[:-1]
        
        if word_len > 1 and word[-1] == 's' and word[-2:] != 'ss':
            return word[:-1]
        
        if word_len > 3 and word[-3:] == 'ing':
            base = word[:-3]
            if len(base) > 1 and base[-1] == base[-2] and base[-1] in self.double_consonants:
                return base[:-1]
            return base
        
        if word_len > 2 and word[-2:] == 'ed':
            base = word[:-2]
            if len(base) > 1 and base[-1] == base[-2] and base[-1] in self.double_consonants:
                return base[:-1]
            return base
        
        if word_len > 2 and word[-2:] == 'ly':
            return word[:-2]
        
        if word_len > 4 and word[-4:] in self.suffix4_set:
            return word[:-4]
        
        return word


# ============ ä¸»è¦çš„TextNormalizerç±» ============

class TextNormalizer:
    """ç»Ÿä¸€çš„æ–‡æœ¬è§„èŒƒåŒ–å™¨ï¼Œè‡ªåŠ¨å¤„ç† NLTK å¯ç”¨æ€§"""
    
    def __init__(self, cache_dir: str = "Meetup/cache"):
        self.processing_times = {}
        self.nltk_available = False
        self.stop_words = set()
        self.stemmer = None
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆé‡å¤§ä¼˜åŒ–ï¼šé¿å…æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°ç¼–è¯‘ï¼‰
        self._compile_regex_patterns()
        
        # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†ç»„ä»¶
        self._initialize_processor()
    
    def _compile_regex_patterns(self):
        """é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½"""
        # ä¸»è¦æ¸…ç†æ¨¡å¼
        self.pattern_clean = re.compile(r'[^a-zA-Z\s\-]')
        
        # ç¼©å†™æ›¿æ¢æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œé¿å…å†²çªï¼‰
        self.pattern_contractions = [
            (re.compile(r"n't\b"), " not"),
            (re.compile(r"'re\b"), " are"),
            (re.compile(r"'ve\b"), " have"),
            (re.compile(r"'ll\b"), " will"),
            (re.compile(r"'d\b"), " would"),
            (re.compile(r"'m\b"), " am"),
            (re.compile(r"'s\b"), ""),  # æœ€åå¤„ç† 'sï¼Œé¿å…ä¸å…¶ä»–æ¨¡å¼å†²çª
        ]
        
        # åˆ†è¯æ¨¡å¼
        self.pattern_tokenize = re.compile(r'\b[a-zA-Z][a-zA-Z\-]+\b')
        
        # è¯å¹²æå–è¾…åŠ©é›†åˆï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
        self.double_consonants = frozenset('bdgmnprt')
        self.suffix2_endings = frozenset(['ch', 'sh', 'ss'])
        self.suffix4_set = frozenset(['ment', 'ness', 'tion'])
    
    def _initialize_processor(self):
        """åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨"""
        # è®¾ç½®åœç”¨è¯
        self._setup_comprehensive_stopwords()
        
        # æ£€æŸ¥NLTKæ˜¯å¦å¯ç”¨
        self._check_nltk_availability()
        
        if self.nltk_available:
            print("ğŸ”¤ ä½¿ç”¨ NLTK æ–‡æœ¬å¤„ç†å™¨")
        else:
            print("ğŸ”¤ ä½¿ç”¨å¢å¼ºå†…ç½®æ–‡æœ¬å¤„ç†å™¨")
    
    def _check_nltk_availability(self):
        """æ£€æŸ¥NLTKæ˜¯å¦å¯ç”¨"""
        try:
            import nltk
            from nltk.tokenize import word_tokenize
            from nltk.corpus import stopwords
            from nltk.stem import PorterStemmer
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å¯ç”¨
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
            
            # åˆå§‹åŒ–NLTKç»„ä»¶
            self.stop_words = set(stopwords.words('english'))
            self.stemmer = PorterStemmer()
            self.nltk_available = True
            
        except (LookupError, ImportError, OSError):
            self.nltk_available = False
    
    def _setup_comprehensive_stopwords(self):
        """è®¾ç½®å…¨é¢çš„åœç”¨è¯åˆ—è¡¨"""
        self.stop_words = {
            'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 
            'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 
            'after', 'above', 'below', 'between', 'among', 'is', 'are', 'was', 'were', 
            'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 
            'would', 'could', 'should', 'may', 'might', 'must', 'can', 'it', 'its', 
            'they', 'them', 'their', 'what', 'which', 'who', 'whom', 'this', 'that', 
            'these', 'those', 'am', 'is', 'are', 'was', 'were', 'been', 'being', 
            'have', 'has', 'had', 'do', 'does', 'did', 'shall', 'will', 'would', 
            'may', 'might', 'must', 'can', 'could', 'should', 'ought', 'i', 'me', 
            'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 
            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 
            'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 
            'themselves'
        }
    
    def _get_cache_key(self, documents: Dict[str, Dict]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹"""
        content = "".join([doc_id + doc_info['content'] for doc_id, doc_info in documents.items()])
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cache_file(self, documents: Dict[str, Dict]) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self._get_cache_key(documents)
        return os.path.join(self.cache_dir, f"normalized_{cache_key}.json")
    
    def load_normalized_docs_from_cache(self, documents: Dict[str, Dict]) -> Dict[str, List[str]]:
        """ä»ç¼“å­˜åŠ è½½è§„èŒƒåŒ–æ–‡æ¡£"""
        cache_file = self._get_cache_file(documents)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'normalized_docs' in data and 'metadata' in data:
                    print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(data['normalized_docs'])} ä¸ªè§„èŒƒåŒ–æ–‡æ¡£")
                    return data['normalized_docs']
            except Exception as e:
                print(f"âŒ åŠ è½½è§„èŒƒåŒ–ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def save_normalized_docs_to_cache(self, normalized_docs: Dict[str, List[str]], documents: Dict[str, Dict]):
        """ä¿å­˜è§„èŒƒåŒ–æ–‡æ¡£åˆ°ç¼“å­˜"""
        cache_file = self._get_cache_file(documents)
        try:
            data = {
                'metadata': {
                    'document_count': len(normalized_docs),
                    'timestamp': time.time(),
                    'processor_type': 'NLTK' if self.nltk_available else 'å†…ç½®'
                },
                'normalized_docs': normalized_docs
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è§„èŒƒåŒ–ç¼“å­˜å¤±è´¥: {e}")
    
    def normalize_text(self, text: str) -> List[str]:
        """æ–‡æœ¬è§„èŒƒåŒ–å¤„ç†æµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼‰"""
        if not text or not text.strip():
            return []
        
        try:
            # 1. è½¬æ¢ä¸ºå°å†™å¹¶æ¸…ç†æ–‡æœ¬
            text = text.lower().strip()
            
            # 2. æ¸…ç†æ–‡æœ¬ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ€§èƒ½æå‡ï¼‰
            text = self.pattern_clean.sub(' ', text)
            
            # 3. åˆ†è¯
            if self.nltk_available:
                tokens = self._nltk_tokenize(text)
            else:
                tokens = self._enhanced_tokenize(text)
            
            # 4. è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯ + è¯å¹²æå–ï¼ˆåˆå¹¶ä¸ºä¸€æ¬¡éå†ï¼‰
            result = []
            if self.nltk_available and self.stemmer:
                for token in tokens:
                    if token not in self.stop_words and len(token) > 2:
                        result.append(self.stemmer.stem(token))
            else:
                for token in tokens:
                    if token not in self.stop_words and len(token) > 2:
                        result.append(self._enhanced_stem(token))
            
            return result
            
        except Exception:
            # å¦‚æœä»»ä½•æ­¥éª¤å¤±è´¥ï¼Œä½¿ç”¨æœ€ç®€åŒ–çš„åˆ†è¯
            return self._minimal_tokenize(text)
    
    def _nltk_tokenize(self, text: str) -> List[str]:
        """ä½¿ç”¨ NLTK åˆ†è¯"""
        try:
            from nltk.tokenize import word_tokenize
            return word_tokenize(text)
        except Exception:
            return self._enhanced_tokenize(text)
    
    def _enhanced_tokenize(self, text: str) -> List[str]:
        """å¢å¼ºçš„åˆ†è¯å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼‰"""
        # å¤„ç†å¸¸è§çš„ç¼©å†™ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„æ¨¡å¼
        for pattern, replacement in self.pattern_contractions:
            text = pattern.sub(replacement, text)
        
        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œåˆ†è¯
        tokens = self.pattern_tokenize.findall(text)
        
        # å¤„ç†è¿å­—ç¬¦å•è¯
        processed_tokens = []
        for token in tokens:
            if '-' in token:
                # åˆ†å‰²è¿å­—ç¬¦å•è¯ï¼Œä½†ä¿ç•™å¸¸è§çš„å¤åˆè¯
                parts = token.split('-')
                if len(parts) == 2 and len(parts[0]) > 2 and len(parts[1]) > 2:
                    # å¯¹äºå¸¸è§çš„å¤åˆè¯ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“å’Œéƒ¨åˆ†
                    processed_tokens.append(token)  # æ•´ä½“
                    processed_tokens.extend([part for part in parts if len(part) > 2])  # éƒ¨åˆ†
                else:
                    processed_tokens.extend([part for part in parts if len(part) > 2])
            else:
                processed_tokens.append(token)
        
        return processed_tokens
    
    def _minimal_tokenize(self, text: str) -> List[str]:
        """æœ€ç®€åŒ–çš„åˆ†è¯æ–¹æ¡ˆï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰"""
        if not text:
            return []
        
        text = text.lower()
        text = self.pattern_clean.sub(' ', text)
        tokens = text.split()
        
        # å¤„ç†è¿å­—ç¬¦
        processed_tokens = []
        for token in tokens:
            if '-' in token:
                parts = [part for part in token.split('-') if len(part) > 2]
                processed_tokens.extend(parts)
            else:
                processed_tokens.append(token)
        
        tokens = [token for token in processed_tokens 
                 if token not in self.stop_words and len(token) > 2]
        
        tokens = [self._enhanced_stem(token) for token in tokens]
        
        return tokens
    
    def _enhanced_stem(self, word: str) -> str:
        """å¢å¼ºçš„è¯å¹²æå–ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨é›†åˆæŸ¥æ‰¾å’Œç¼“å­˜ï¼‰"""
        word_len = len(word)
        
        if word_len <= 3:
            return word
        
        # å¤„ç†å¤æ•°å½¢å¼ -ies
        if word_len > 3 and word[-3:] == 'ies':
            return word[:-3] + 'y'
        
        # å¤„ç† -es ç»“å°¾
        if word_len > 2 and word[-2:] == 'es':
            base = word[:-2]
            base_len = len(base)
            # ä½¿ç”¨é›†åˆæŸ¥æ‰¾ä¼˜åŒ–æ€§èƒ½
            if base_len >= 2 and base[-2:] in self.suffix2_endings:
                return base
            if base_len >= 1 and base[-1] in 'sxz':
                return base
            return word[:-1]
        
        # å¤„ç† -s ç»“å°¾ï¼ˆé -ssï¼‰
        if word_len > 1 and word[-1] == 's' and word[-2:] != 'ss':
            return word[:-1]
        
        # å¤„ç† -ing ç»“å°¾
        if word_len > 3 and word[-3:] == 'ing':
            base = word[:-3]
            base_len = len(base)
            # åŒå†™è¾…éŸ³å­—æ¯è§„åˆ™ï¼šä½¿ç”¨é›†åˆæŸ¥æ‰¾
            if base_len > 1 and base[-1] == base[-2] and base[-1] in self.double_consonants:
                return base[:-1]
            return base
        
        # å¤„ç† -ed ç»“å°¾
        if word_len > 2 and word[-2:] == 'ed':
            base = word[:-2]
            base_len = len(base)
            # åŒå†™è¾…éŸ³å­—æ¯è§„åˆ™
            if base_len > 1 and base[-1] == base[-2] and base[-1] in self.double_consonants:
                return base[:-1]
            return base
        
        # å¤„ç† -ly ç»“å°¾
        if word_len > 2 and word[-2:] == 'ly':
            return word[:-2]
        
        # å¤„ç†åè¯åç¼€ï¼šä½¿ç”¨é›†åˆæŸ¥æ‰¾
        if word_len > 4 and word[-4:] in self.suffix4_set:
            return word[:-4]
        
        return word
    
    def process_documents(self, documents: Dict[str, Dict], use_cache: bool = True, use_multiprocessing: bool = True) -> Dict[str, List[str]]:
        """å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œï¼‰"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_docs = self.load_normalized_docs_from_cache(documents)
            if cached_docs is not None:
                return cached_docs
        
        start_time = time.time()
        total_docs = len(documents)
        
        processor_type = "NLTK" if self.nltk_available else "å¢å¼ºå†…ç½®"
        
        # å†³å®šæ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
        use_parallel = use_multiprocessing and total_docs > 100 and cpu_count() > 1
        
        if use_parallel:
            num_workers = min(cpu_count(), 8)  # æœ€å¤šä½¿ç”¨8ä¸ªè¿›ç¨‹
            print(f"ğŸ”¤ æ–‡æœ¬è§„èŒƒåŒ–å¤„ç† (ä½¿ç”¨{processor_type}å¤„ç†å™¨ï¼Œ{num_workers}è¿›ç¨‹å¹¶è¡Œ)")
            normalized_docs = self._process_parallel(documents, num_workers)
        else:
            print(f"ğŸ”¤ æ–‡æœ¬è§„èŒƒåŒ–å¤„ç† (ä½¿ç”¨{processor_type}å¤„ç†å™¨ï¼Œå•è¿›ç¨‹)")
            normalized_docs = self._process_sequential(documents)
        
        end_time = time.time()
        self.processing_times['text_normalization'] = end_time - start_time
        
        print(f"âœ… æ–‡æœ¬è§„èŒƒåŒ–å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f}ç§’")
        
        # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆæ³¨æ„ï¼šå‚æ•°é¡ºåºæ˜¯ normalized_docs, documentsï¼‰
        if use_cache:
            self.save_normalized_docs_to_cache(normalized_docs, documents)
        
        return normalized_docs
    
    def _process_sequential(self, documents: Dict[str, Dict]) -> Dict[str, List[str]]:
        """é¡ºåºå¤„ç†æ–‡æ¡£"""
        normalized_docs = {}
        
        progress_bar = tqdm(
            documents.items(),
            desc="ğŸ”¤ æ–‡æœ¬è§„èŒƒåŒ–",
            total=len(documents),
            unit="æ–‡æ¡£",
            ncols=100,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
        
        for doc_id, doc_info in progress_bar:
            content = doc_info['content']
            normalized_docs[doc_id] = self.normalize_text(content)
        
        progress_bar.close()
        return normalized_docs
    
    def _process_parallel(self, documents: Dict[str, Dict], num_workers: int) -> Dict[str, List[str]]:
        """å¹¶è¡Œå¤„ç†æ–‡æ¡£ï¼ˆå¤šè¿›ç¨‹ï¼‰"""
        # å‡†å¤‡æ•°æ®ï¼šè½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åˆ†å—
        doc_items = list(documents.items())
        
        # ä½¿ç”¨multiprocessingè¿›è¡Œå¹¶è¡Œå¤„ç†
        with Pool(processes=num_workers) as pool:
            # ä½¿ç”¨imap_unorderedä»¥è·å¾—æ›´å¥½çš„è¿›åº¦åé¦ˆ
            results = list(tqdm(
                pool.imap_unordered(
                    _process_single_document,
                    doc_items,
                    chunksize=max(1, len(doc_items) // (num_workers * 4))
                ),
                total=len(doc_items),
                desc="ğŸ”¤ æ–‡æœ¬è§„èŒƒåŒ–",
                unit="æ–‡æ¡£",
                ncols=100,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            ))
        
        # è½¬æ¢å›å­—å…¸
        return dict(results)
    
    def get_processing_times(self) -> Dict:
        """è·å–å¤„ç†æ—¶é—´ç»Ÿè®¡"""
        return self.processing_times